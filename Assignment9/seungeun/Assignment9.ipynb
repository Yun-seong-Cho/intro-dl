{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2024 Winter Introduction to Deep Learning\n",
        "### Based on Prof. Oh's Youtube Lecture\n",
        "https://youtube.com/playlist?list=PLvbUC2Zh5oJvByu9KL82bswYT2IKf0K1M\n",
        "\n",
        "> Assignment #9\n",
        "\n",
        "\n",
        "*   Youtube Lecture #36-40\n",
        "*   Written by Seungeun Lee"
      ],
      "metadata": {
        "id": "YcXb0XLEvHns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Vanilla GAN\n",
        "*     Reference. https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/generative_adversarial_network/main.py, https://github.com/Yangyangii/pytorch-practice/blob/master/GAN.ipynb, https://arxiv.org/pdf/1406.2661"
      ],
      "metadata": {
        "id": "qM2kM5G7BYZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "import sys\n",
        "from matplotlib.pyplot import imshow, imsave\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "11Ih6xyA5NoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'GAN'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "LF3Dn5b_5Ni2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_image(G, n_noise):\n",
        "    \"\"\"\n",
        "        save sample 100 images\n",
        "    \"\"\"\n",
        "    z = torch.randn(100, n_noise).to(DEVICE)\n",
        "    y_hat = G(z).view(100, 28, 28) # (100, 28, 28)\n",
        "    result = y_hat.cpu().data.numpy()\n",
        "    img = np.zeros([280, 280])\n",
        "    for j in range(10):\n",
        "        img[j*28:(j+1)*28] = np.concatenate([x for x in result[j*10:(j+1)*10]], axis=-1)\n",
        "    return img"
      ],
      "metadata": {
        "id": "ipCaOFjQ7Y_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple Discriminator w/ MLP\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=784, num_classes=1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, num_classes),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_ = x.view(x.size(0), -1)\n",
        "        y_ = self.layer(y_)\n",
        "        return y_"
      ],
      "metadata": {
        "id": "ftRUJDWQ7Y4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple Generator w/ MLP\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=100, num_classes=784):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, num_classes),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_ = self.layer(x)\n",
        "        y_ = y_.view(x.size(0), 1, 28, 28)\n",
        "        return y_"
      ],
      "metadata": {
        "id": "qmUmWqV-7Yye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_noise = 100"
      ],
      "metadata": {
        "id": "w0XeLU8s7Ytd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = Discriminator().to(DEVICE)\n",
        "G = Generator(n_noise).to(DEVICE)"
      ],
      "metadata": {
        "id": "MaFANjmp7Yn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])"
      ],
      "metadata": {
        "id": "6bx146Jg7YiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = datasets.MNIST(root='../data/', train=True, transform=transform, download=True)"
      ],
      "metadata": {
        "id": "ivGBFM0F9BaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "data_loader = DataLoader(dataset=mnist, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "38sHoNs1bxTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "G_opt = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "jlkqDPyxbxOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epoch = 50 # need more than 10 epochs for training generator\n",
        "step = 0\n",
        "n_critic = 1 # for training more k steps about Discriminator"
      ],
      "metadata": {
        "id": "ehnx8keVcaoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D_labels = torch.ones(batch_size, 1).to(DEVICE) # Discriminator Label to real\n",
        "D_fakes = torch.zeros(batch_size, 1).to(DEVICE) # Discriminator Label to fake"
      ],
      "metadata": {
        "id": "kyfoJQKtcjfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('samples'):\n",
        "    os.makedirs('samples')"
      ],
      "metadata": {
        "id": "NvR9fTjEkYPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(max_epoch):\n",
        "    for idx, (images, _) in enumerate(data_loader):\n",
        "        # Training Discriminator\n",
        "        x = images.to(DEVICE)\n",
        "        x_outputs = D(x)\n",
        "        D_x_loss = criterion(x_outputs, D_labels)\n",
        "\n",
        "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
        "        z_outputs = D(G(z))\n",
        "        D_z_loss = criterion(z_outputs, D_fakes)\n",
        "        D_loss = D_x_loss + D_z_loss\n",
        "\n",
        "        D.zero_grad()\n",
        "        D_loss.backward()\n",
        "        D_opt.step()\n",
        "\n",
        "        if step % n_critic == 0:\n",
        "            # Training Generator\n",
        "            z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
        "            z_outputs = D(G(z))\n",
        "            G_loss = criterion(z_outputs, D_labels)\n",
        "\n",
        "            G.zero_grad()\n",
        "            G_loss.backward()\n",
        "            G_opt.step()\n",
        "\n",
        "        if step % 500 == 0:\n",
        "            print('Epoch: {}/{}, Step: {}, D Loss: {}, G Loss: {}'.format(epoch, max_epoch, step, D_loss.item(), G_loss.item()))\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            G.eval()\n",
        "            img = get_sample_image(G, n_noise)\n",
        "            imsave('samples/{}_step{}.jpg'.format(MODEL_NAME, str(step).zfill(3)), img, cmap='gray')\n",
        "            G.train()\n",
        "        step += 1"
      ],
      "metadata": {
        "id": "uUU-rcKBkct-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generation to image\n",
        "G.eval()\n",
        "imshow(get_sample_image(G, n_noise), cmap='gray')"
      ],
      "metadata": {
        "id": "M4EIIJC4kcmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving params.\n",
        "torch.save(D.state_dict(), 'D.pkl')\n",
        "torch.save(G.state_dict(), 'G.pkl')"
      ],
      "metadata": {
        "id": "5vUWbFHLnBZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1. Explain the code of the '1. Vanilla GAN' section \"briefly\"."
      ],
      "metadata": {
        "id": "O3WZDNNDcrlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RaEAvsXLnbWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YRaovAjtnbQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Congratulations on finishing the \"Introduction the Deep Learning\"!!!\n",
        "## Now you're a deep learning expert too!"
      ],
      "metadata": {
        "id": "Ai-zS87adcvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### After this session, if you're interested in further study, I recommend exploring the following papers and codes:\n",
        "\n",
        "(1) [Computer Vision] U-GAT-IT, Vision Transformer, (Mask, Fast, Faster) R-CNN, YOLO, Detectron(2), nnUnet, MobileNet, Neural Style Transfer\n",
        "\n",
        "(2) [Natural Language Processing] KoBERT, BERT, KoBART, BART, ELECTRA\n",
        "\n",
        "(3) [Audio] ResNeXt, ResMax, AASIST\n",
        "\n",
        "(4) [Learning Technique] Curriculum Learning, Moco Encoder, Contrastive Learning, Student-teacher Model, Knowledge Distillation, Meta Learning\n",
        "\n",
        "(5) [Machine Learning for Tabular data Analysis] SVM, LR, XGBoost, LightGBM, CatBoost, TabNet, AutoML (optuna)\n",
        "\n",
        "Feel free to delve into these topics to deepen your understanding in various areas of AI! If you have any questions about specific topics or need more information, feel free to ask."
      ],
      "metadata": {
        "id": "FjZfH615nmpk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wx026812BNuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFCJ_YjpCn9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The End."
      ],
      "metadata": {
        "id": "spc48rNHQnYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Please upload your Colab file @Github https://github.com/duneag2/intro-dl/tree/main/Assignment8\n",
        "\n",
        "*   First, make your folder by your name (e.g. seungeun)\n",
        "*   Then upload your \"Jupyter Notebook\" file under that directory\n",
        "\n",
        "###### Need Help?\n",
        "\n",
        "\n",
        "\n",
        "*   Please refer to this link https://yeko90.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC-colab%EC%BD%94%EB%9E%A9%EC%97%90%EC%84%9C-%EC%95%95%EC%B6%95%ED%8C%8C%EC%9D%BC-%ED%92%80%EA%B8%B0 OR\n",
        "*   Just save your Jupyter Notebook (.ipynb) file in here (colab) and upload via 'Add file' - 'Upload files' https://nthree.tistory.com/60"
      ],
      "metadata": {
        "id": "iMNBVkjiS7D9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XzVGuer0S9Oh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}