{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2024 Winter Introduction to Deep Learning\n",
        "### Based on Prof. Oh's Youtube Lecture\n",
        "https://youtube.com/playlist?list=PLvbUC2Zh5oJvByu9KL82bswYT2IKf0K1M\n",
        "\n",
        "> Assignment #3\n",
        "\n",
        "\n",
        "*   Youtube Lecture #12-16\n",
        "*   Written by Seungeun Lee"
      ],
      "metadata": {
        "id": "YcXb0XLEvHns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. PyTorch Optimizer\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qM2kM5G7BYZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*    Reference. https://gr-st-dev.tistory.com/151\n",
        "*    The code below will not be executed properly."
      ],
      "metadata": {
        "id": "l-eq03krP21I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN THIS CODE!\n",
        "# (1) SGD Optimizer\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "model = MyModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001)\n",
        "\n",
        "# For every epoch\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()  # Weight Initialization\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Weight update w/ optimizer"
      ],
      "metadata": {
        "id": "akDgspj1ptpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN THIS CODE!\n",
        "# (2) Adam Optimizer\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "model = MyModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.01)\n",
        "# We usually use Adam for many tasks - almost a \"DEFAULT\" optimizer\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "5kLHcDO9p1aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN THIS CODE!\n",
        "# (3) RMSProp Optimizer\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "model = MyModel()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9, weight_decay=0.01)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "TSl6zem51bVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1. Write a detailed explanation for\n",
        "*     optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.01)\n",
        "*     Hint: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"
      ],
      "metadata": {
        "id": "JtrlgMOov47e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vD09VxoAptUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4daoEj3-ptjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Revisiting Assignment1 MLP\n",
        "\n",
        "*   Reference. https://github.com/Justin-A/DeepLearning101/blob/master/2-1_MNIST_MLP.ipynb\n",
        "\n"
      ],
      "metadata": {
        "id": "1BgQvGP205Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets"
      ],
      "metadata": {
        "id": "P8ULcApY1FIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GPU (Optional)\n",
        "# 런타임 - 런타임 유형 변경 - T4 GPU\n",
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "PSqnwiKKH_-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
      ],
      "metadata": {
        "id": "S4rG0VQZ4C7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "46yjJnH14C00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
        "                               train = True,\n",
        "                               download = True,\n",
        "                               transform = transforms.ToTensor())\n",
        "\n",
        "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
        "                              train = False,\n",
        "                              transform = transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = BATCH_SIZE,\n",
        "                                           shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                          batch_size = BATCH_SIZE,\n",
        "                                          shuffle = False)"
      ],
      "metadata": {
        "id": "AfStirAO4T6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (X_train, y_train) in train_loader:\n",
        "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
        "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
        "    break"
      ],
      "metadata": {
        "id": "077rWr545YA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pltsize = 1\n",
        "plt.figure(figsize=(10 * pltsize, pltsize))\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
        "    plt.title('Class: ' + str(y_train[i].item()))"
      ],
      "metadata": {
        "id": "ig5K7tkZ45FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Multi Layer Perceptron '''\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = F.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.sigmoid(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Kbx__bbt5uK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Optimizer, Objective Function '''\n",
        "model = Net().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "fQC4PVvp6JTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch : 0.95 ** epoch)"
      ],
      "metadata": {
        "id": "NuBZapjnvT4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(image),\n",
        "                len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
        "                loss.item()))\n",
        "    scheduler.step() # Updating learning rate"
      ],
      "metadata": {
        "id": "EZPMsVoY6ZmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_loader:\n",
        "            image = image.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            output = model(image)\n",
        "            test_loss += criterion(output, label).item()\n",
        "            prediction = output.max(1, keepdim = True)[1]\n",
        "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
        "\n",
        "\n",
        "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "dgWR5Np66mLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, train_loader, optimizer, log_interval = 200)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
        "        epoch, test_loss, test_accuracy))"
      ],
      "metadata": {
        "id": "1K_56c6sq5da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2. Optimizer and Learning rate Scheduler\n",
        "#####(1) Change the Net model's optimizer to Adam optimizer and run the code.\n",
        "#####(2) What is learning rate scheduler? Can you change the type of the scheduler in the code above? Also, write a detailed explanation of the scheduler to choose.\n",
        "##### Hint: https://sanghyu.tistory.com/113, https://dacon.io/codeshare/2373, and https://gaussian37.github.io/dl-pytorch-lr_scheduler/"
      ],
      "metadata": {
        "id": "E3Ry6IWL7mTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Overfitting"
      ],
      "metadata": {
        "id": "RHoAQdMP1L4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Although we usually build DL models based on PyTorch,\n",
        "# we sometimes need to use TensorFlow upon your colleagues' requests"
      ],
      "metadata": {
        "id": "1nudN6YK1PS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's take a look at the overfitting problem w/ Tensorflow"
      ],
      "metadata": {
        "id": "aObCDbls1SId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference. https://www.tensorflow.org/tutorials/keras/overfit_and_underfit?hl=ko\n",
        "### Please read the document and run the following code!"
      ],
      "metadata": {
        "id": "kuvc-V-uUyGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "zCg8EJLdE_8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/tensorflow/docs ## installing tensorflow's official github\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.modeling\n",
        "import tensorflow_docs.plots"
      ],
      "metadata": {
        "id": "YmVVkwqfVS__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from  IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pathlib\n",
        "import shutil\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "GP9NJ03qVS6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
        "shutil.rmtree(logdir, ignore_errors=True)"
      ],
      "metadata": {
        "id": "x4gAjLBrVS1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'http://mlphysics.ics.uci.edu/data/higgs/HIGGS.csv.gz') ## it takes quite a long time to download the data"
      ],
      "metadata": {
        "id": "A4jpLlXQVSv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURES = 28"
      ],
      "metadata": {
        "id": "HzNEM_SzVecD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type=\"GZIP\")"
      ],
      "metadata": {
        "id": "T3wqWI38Velp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pack_row(*row):\n",
        "  label = row[0]\n",
        "  features = tf.stack(row[1:],1)\n",
        "  return features, label"
      ],
      "metadata": {
        "id": "Qio5Z5GHVerK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "packed_ds = ds.batch(10000).map(pack_row).unbatch()"
      ],
      "metadata": {
        "id": "7uGQjdd7VewJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for features,label in packed_ds.batch(1000).take(1):\n",
        "  print(features[0])\n",
        "  plt.hist(features.numpy().flatten(), bins = 101)"
      ],
      "metadata": {
        "id": "32gL2LurVmTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_VALIDATION = int(1e3)\n",
        "N_TRAIN = int(1e4)\n",
        "BUFFER_SIZE = int(1e4)\n",
        "BATCH_SIZE = 500\n",
        "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
      ],
      "metadata": {
        "id": "PDLR4-YoVmMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
        "train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()"
      ],
      "metadata": {
        "id": "8skXdwekVmHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "id": "lcp3jEdUVmA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate_ds = validate_ds.batch(BATCH_SIZE)\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "FDFcb54OVsmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "  0.001,\n",
        "  decay_steps=STEPS_PER_EPOCH*1000,\n",
        "  decay_rate=1,\n",
        "  staircase=False)\n",
        "\n",
        "def get_optimizer():\n",
        "  return tf.keras.optimizers.Adam(lr_schedule)"
      ],
      "metadata": {
        "id": "wfDnT-KOVse2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = np.linspace(0,100000)\n",
        "lr = lr_schedule(step)\n",
        "plt.figure(figsize = (8,6))\n",
        "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "_ = plt.ylabel('Learning Rate')"
      ],
      "metadata": {
        "id": "R2LETNbgVsYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_callbacks(name):\n",
        "  return [\n",
        "    tfdocs.modeling.EpochDots(),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
        "    tf.keras.callbacks.TensorBoard(logdir/name),\n",
        "  ]"
      ],
      "metadata": {
        "id": "xZGGbso2VsT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_and_fit(model, name, optimizer=None, max_epochs=10000):\n",
        "  if optimizer is None:\n",
        "    optimizer = get_optimizer()\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                metrics=[\n",
        "                  tf.keras.losses.BinaryCrossentropy(\n",
        "                      from_logits=True, name='binary_crossentropy'),\n",
        "                  'accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(\n",
        "    train_ds,\n",
        "    steps_per_epoch = STEPS_PER_EPOCH,\n",
        "    epochs=max_epochs,\n",
        "    validation_data=validate_ds,\n",
        "    callbacks=get_callbacks(name),\n",
        "    verbose=0)\n",
        "  return history"
      ],
      "metadata": {
        "id": "Dkj4rGS7VsOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_model = tf.keras.Sequential([\n",
        "    layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "1ViuCYnzWDqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_histories = {}"
      ],
      "metadata": {
        "id": "9olxMDpuWDky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_histories['Tiny'] = compile_and_fit(tiny_model, 'sizes/Tiny')"
      ],
      "metadata": {
        "id": "zQhwmTUeWDfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
        "plotter.plot(size_histories)\n",
        "plt.ylim([0.5, 0.7])"
      ],
      "metadata": {
        "id": "m2BTt-1gWDWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_model = tf.keras.Sequential([\n",
        "    # `input_shape` is only required here so that `.summary` works.\n",
        "    layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(16, activation='elu'),\n",
        "    layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "DYdAUa-7WKE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_histories['Small'] = compile_and_fit(small_model, 'sizes/Small')"
      ],
      "metadata": {
        "id": "Q7hxoXJPWJuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medium_model = tf.keras.Sequential([\n",
        "    layers.Dense(64, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(64, activation='elu'),\n",
        "    layers.Dense(64, activation='elu'),\n",
        "    layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "u4hsknghWJm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_histories['Medium']  = compile_and_fit(medium_model, \"sizes/Medium\")"
      ],
      "metadata": {
        "id": "eNP2rmytJuza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "large_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "r5bI9g0LJus7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_histories['large'] = compile_and_fit(large_model, \"sizes/large\")"
      ],
      "metadata": {
        "id": "8cTilzlCJujR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotter.plot(size_histories)\n",
        "a = plt.xscale('log')\n",
        "plt.xlim([5, max(plt.xlim())])\n",
        "plt.ylim([0.5, 0.7])\n",
        "plt.xlabel(\"Epochs [Log Scale]\")"
      ],
      "metadata": {
        "id": "xwhGmK_wJucT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## New Tool: TensorBoard\n",
        "## Please refer to https://tutorials.pytorch.kr/recipes/recipes/tensorboard_with_pytorch.html\n",
        "## and https://tutorials.pytorch.kr/intermediate/tensorboard_tutorial.html"
      ],
      "metadata": {
        "id": "6nJPKvgPNTIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docs_infra: no_execute\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Open an embedded TensorBoard viewer\n",
        "%tensorboard --logdir {logdir}/sizes"
      ],
      "metadata": {
        "id": "GdcjweDVJuW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97\",\n",
        "    width=\"100%\", height=\"800px\")"
      ],
      "metadata": {
        "id": "uRy3vgAYJuQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*     How to avoid Overfitting"
      ],
      "metadata": {
        "id": "5NkbWZokKFfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree(logdir/'regularizers/Tiny', ignore_errors=True)\n",
        "shutil.copytree(logdir/'sizes/Tiny', logdir/'regularizers/Tiny')"
      ],
      "metadata": {
        "id": "zZDk_euJJuH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regularizer_histories = {}\n",
        "regularizer_histories['Tiny'] = size_histories['Tiny']"
      ],
      "metadata": {
        "id": "gXgkpFaFJuBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## L2 Regularization\n",
        "l2_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001),\n",
        "                 input_shape=(FEATURES,)),\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "regularizer_histories['l2'] = compile_and_fit(l2_model, \"regularizers/l2\")"
      ],
      "metadata": {
        "id": "NdeBKythKSmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotter.plot(regularizer_histories)\n",
        "plt.ylim([0.5, 0.7])"
      ],
      "metadata": {
        "id": "OgrZu1YGKSc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = l2_model(features)\n",
        "regularization_loss=tf.add_n(l2_model.losses)"
      ],
      "metadata": {
        "id": "hU4rMUcGKSWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Dropout\n",
        "dropout_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "regularizer_histories['dropout'] = compile_and_fit(dropout_model, \"regularizers/dropout\")"
      ],
      "metadata": {
        "id": "N5_UXJEsKSPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotter.plot(regularizer_histories)\n",
        "plt.ylim([0.5, 0.7])"
      ],
      "metadata": {
        "id": "9VcqOEwNKSFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## L2 + Dropout\n",
        "combined_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "regularizer_histories['combined'] = compile_and_fit(combined_model, \"regularizers/combined\")"
      ],
      "metadata": {
        "id": "oE2yKcqkKR7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotter.plot(regularizer_histories)\n",
        "plt.ylim([0.5, 0.7])"
      ],
      "metadata": {
        "id": "onoWlIrRKzGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir {logdir}/regularizers"
      ],
      "metadata": {
        "id": "8XV0A7uaK0sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/fGInKDo8TXes1z7HQku9mw/#scalars&_smoothingWeight=0.97\",\n",
        "    width = \"100%\",\n",
        "    height=\"800px\")"
      ],
      "metadata": {
        "id": "E81SfJHtK0hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ApBpQ1jSK0bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_-TeEjhIK0T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TGO-htu3K0H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3.\n",
        "##### Please write a code for L1 Regularization + Dropout Model and run it!"
      ],
      "metadata": {
        "id": "xk4dFsyw24RV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BsiXvSCx21ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PtxmAAqCWr2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21_zoHW4ZpaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The End."
      ],
      "metadata": {
        "id": "spc48rNHQnYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Please upload your Colab file @Github https://github.com/duneag2/intro-dl/tree/main/Assignment4\n",
        "\n",
        "*   First, make your folder by your name (e.g. seungeun)\n",
        "*   Then upload your \"Jupyter Notebook\" file under that directory\n",
        "\n",
        "###### Need Help?\n",
        "\n",
        "\n",
        "\n",
        "*   Please refer to this link https://yeko90.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC-colab%EC%BD%94%EB%9E%A9%EC%97%90%EC%84%9C-%EC%95%95%EC%B6%95%ED%8C%8C%EC%9D%BC-%ED%92%80%EA%B8%B0 OR\n",
        "*   Just save your Jupyter Notebook (.ipynb) file in here (colab) and upload via 'Add file' - 'Upload files' https://nthree.tistory.com/60"
      ],
      "metadata": {
        "id": "iMNBVkjiS7D9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XzVGuer0S9Oh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}