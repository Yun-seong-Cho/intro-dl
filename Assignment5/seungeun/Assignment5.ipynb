{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 2024 Winter Introduction to Deep Learning\n","### Based on Prof. Oh's Youtube Lecture\n","https://youtube.com/playlist?list=PLvbUC2Zh5oJvByu9KL82bswYT2IKf0K1M\n","\n","> Assignment #5\n","\n","\n","*   Youtube Lecture #17-22\n","*   Written by Seungeun Lee"],"metadata":{"id":"YcXb0XLEvHns"}},{"cell_type":"markdown","source":["## 1. Batch Normalization\n","\n","\n"],"metadata":{"id":"qM2kM5G7BYZr"}},{"cell_type":"markdown","source":["*    Reference. https://huangdi.tistory.com/m/9, https://wegonnamakeit.tistory.com/m/47, https://wikidocs.net/195419\n","*    For https://huangdi.tistory.com/m/9, try to understand the need for Batch Normalization. (The code is not running properly due to the github issues)"],"metadata":{"id":"l-eq03krP21I"}},{"cell_type":"code","source":[" # DO NOT run this code!!\n","# nn layers\n","linear1 = torch.nn.Linear(784, 32, bias=True)\n","linear2 = torch.nn.Linear(32, 32, bias=True)\n","linear3 = torch.nn.Linear(32, 10, bias=True)\n","\n","relu = torch.nn.ReLU()\n","bn1 = torch.nn.BatchNorm1d(32) # Batch Normalization for 1D data\n","bn2 = torch.nn.BatchNorm1d(32)\n","\n","nn_linear1 = torch.nn.Linear(784, 32, bias=True)\n","nn_linear2 = torch.nn.Linear(32, 32, bias=True) nn_linear3 = torch.nn.Linear(32, 10, bias=True)\n","\n","\n","# model\n","# torch.nn.Seqeuntial is kind of a \"stack\" operation of layers\n","# We usually apply ReLU after the BN, but it's not a must!\n","bn_model = torch.nn.Sequential(linear1, bn1, relu,\n","                              linear2, bn2, relu,\n","                              linear3).to(device)\n","# Model w.o. BN\n","# Linear + ReLU is a convention\n","nn_model = torch.nn.Sequential(nn_linear1, relu,\n","                               nn_linear2, relu,\n","                               nn_linear3).to(device)"],"metadata":{"id":"Q7_JWyTWKoyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If not using torch.nn.Sequential, you can use class module!\n","\n","import torch.nn as nn\n","\n","class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        # in __init__, initialize the layers you'd like to use\n","        self.fc1 = nn.Linear(100, 50)\n","        self.bn = nn.BatchNorm1d(num_features=50)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(50, 10)\n","\n","    def forward(self, x):\n","        # in forward, place the layers sequentially\n","        x = self.fc1(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"k6bdnbKmK-ZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BN 1D\n","m = nn.BatchNorm1d(100)\n","input = torch.randn(20, 100)\n","output = m(input)\n","\n","# BN 2D\n","# So far, we've only used BN 1D\n","# But what's BN 2D??\n","m = nn.BatchNorm2d(100)\n","input = torch.randn(20, 100, 35, 45)\n","output = m(input)"],"metadata":{"id":"mh9UvwnlK-3G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Question 1. Compare & Contrast BatchNorm1d and Batchnorm2d\n","*     Hint: https://gaussian37.github.io/dl-concept-batchnorm/, https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html, and https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html"],"metadata":{"id":"JtrlgMOov47e"}},{"cell_type":"code","source":[],"metadata":{"id":"vD09VxoAptUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4daoEj3-ptjd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Weight Initialization\n","\n","*   Reference. https://ysg2997.tistory.com/14, https://m.blog.naver.com/PostView.naver?blogId=tinz6461&logNo=221599717016&proxyReferer="],"metadata":{"id":"1BgQvGP205Tf"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.init as init"],"metadata":{"id":"dgxRjABUI1sv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Quick Question\n","# Read the document above and write a line-by-line\n","# explanation of the code below."],"metadata":{"id":"0gyoVqzlceii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN,self).__init__()\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(1,16,3,padding=1),  # 28 x 28\n","            nn.ReLU(),\n","            nn.Conv2d(16,32,3,padding=1), # 28 x 28\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),            # 14 x 14\n","            nn.Conv2d(32,64,3,padding=1), # 14 x 14\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2)             #  7 x 7\n","        )\n","        self.fc_layer = nn.Sequential(\n","            nn.Linear(64*7*7,100),\n","            nn.ReLU(),\n","            nn.Linear(100,10)\n","        )\n","\n","            if isinstance(m, nn.Conv2d):\n","\n","                init.kaiming_normal_(m.weight.data)\n","                m.bias.data.fill_(0)\n","\n","            elif isinstance(m, nn.Linear):\n","\n","                init.kaiming_normal_(m.weight.data)\n","                m.bias.data.fill_(0)\n","\n","    def forward(self,x):\n","        out = self.layer(x)\n","        out = out.view(batch_size,-1)\n","        out = self.fc_layer(out)\n","        return out"],"metadata":{"id":"P8ULcApY1FIY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Then let's look at the efficiency of such Weight Init. techniques\n","# with visualization"],"metadata":{"id":"ZaezvQcYc7TY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Quick Question\n","# Read the document above and briefly explain the code below."],"metadata":{"id":"8ajqLBL6dnAj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# sigmoid\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# ReLU\n","def ReLU(x):\n","    return np.maximum(0, x)\n","\n","# tanh\n","def tanh(x):\n","    return np.tanh(x)\n","\n","def weight_init(method=None):\n","    w = 0\n","    if method == 'large':\n","        w = np.random.randn(node_num, node_num) * 1\n","    elif method == 'small':\n","        w = np.random.randn(node_num, node_num) * 0.01\n","    elif method == 'xavier':\n","        w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)  # Xavier init\n","    elif method == 'he':\n","        w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)  # He init\n","\n","    return w"],"metadata":{"id":"6J-FjScBKF7y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_data = np.random.randn(1000, 100)\n","node_num = 100\n","hidden_layer_size = 5\n","activations = {}\n","\n","x = input_data\n","\n","for i in range(hidden_layer_size):\n","    if i != 0:\n","        x = activations[i-1]\n","\n","    w = weight_init('small')\n","    a = np.dot(x, w)\n","\n","    # z = sigmoid(a)\n","    # z = ReLU(a)\n","    z = tanh(a)\n","\n","    activations[i] = z\n","\n","for i, a in activations.items():\n","    plt.subplot(1, len(activations), i+1)\n","    plt.title(str(i+1) + \"-layer\")\n","    if i != 0: plt.yticks([], [])\n","#     plt.xlim(0.1, 1)\n","    plt.ylim(0, 8000)\n","    plt.hist(a.flatten(), 30, range=(-1,1))\n","plt.show()"],"metadata":{"id":"elzVHh2FKHa2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(hidden_layer_size):\n","    if i != 0:\n","        x = activations[i-1]\n","\n","    w = weight_init('large')\n","    a = np.dot(x, w)\n","\n","    # z = sigmoid(a)\n","    # z = ReLU(a)\n","    z = tanh(a)\n","\n","    activations[i] = z\n","\n","for i, a in activations.items():\n","    plt.subplot(1, len(activations), i+1)\n","    plt.title(str(i+1) + \"-layer\")\n","    if i != 0: plt.yticks([], [])\n","#     plt.xlim(0.1, 1)\n","    plt.ylim(0, 7000)\n","    plt.hist(a.flatten(), 30, range=(-1,1))\n","plt.show()"],"metadata":{"id":"a91BiqWZKHCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Xavier Initialization\n","input_data = np.random.randn(1000, 100)\n","node_num = 100\n","hidden_layer_size = 5\n","activations = {}\n","\n","x = input_data\n","\n","for i in range(hidden_layer_size):\n","    if i != 0:\n","        x = activations[i-1]\n","\n","    w = weight_init('xavier')\n","    a = np.dot(x, w)\n","\n","    # z = sigmoid(a)\n","    # z = ReLU(a)\n","    z = tanh(a)\n","\n","    activations[i] = z\n","\n","for i, a in activations.items():\n","    plt.subplot(1, len(activations), i+1)\n","    plt.title(str(i+1) + \"-layer\")\n","    if i != 0: plt.yticks([], [])\n","#     plt.xlim(0.1, 1)\n","    plt.ylim(0, 7000)\n","    plt.hist(a.flatten(), 30, range=(-1,1))\n","plt.show()"],"metadata":{"id":"mXBo8IINKUjp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# He Initialization\n","input_data = np.random.randn(1000, 100)\n","node_num = 100\n","hidden_layer_size = 5\n","activations = {}\n","\n","x = input_data\n","\n","for i in range(hidden_layer_size):\n","    if i != 0:\n","        x = activations[i-1]\n","\n","    w = weight_init('xavier')\n","    a = np.dot(x, w)\n","\n","    # z = sigmoid(a)\n","    z = ReLU(a)\n","    # z = tanh(a)\n","\n","    activations[i] = z\n","\n","for i, a in activations.items():\n","    plt.subplot(1, len(activations), i+1)\n","    plt.title(str(i+1) + \"-layer\")\n","    if i != 0: plt.yticks([], [])\n","#     plt.xlim(0.1, 1)\n","    plt.ylim(0, 7000)\n","    plt.hist(a.flatten(), 30, range=(0,1))\n","plt.show()"],"metadata":{"id":"O-eTjk0mKYJN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. CNN\n","*     Reference. https://justkode.kr/deep-learning/pytorch-cnn/"],"metadata":{"id":"RHoAQdMP1L4b"}},{"cell_type":"code","source":["## GPU (Optional)\n","# 런타임 - 런타임 유형 변경 - T4 GPU\n","# !nvidia-smi"],"metadata":{"id":"PSqnwiKKH_-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Question\n","# Run two cells below and try to explain the\n","# change of tensor size after passing through the layers\n","# e.g.) Conv1: [10, 1, 20, 20] -> [10, 3, 16, 16], because --"],"metadata":{"id":"rBRJM0HOfOVM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","  def __init__(self):\n","    super(CNN, self).__init__()\n","    self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5, stride=1)\n","    self.conv2 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=5, stride=1)\n","    self.fc1 = nn.Linear(10 * 12 * 12, 50)\n","    self.fc2 = nn.Linear(50, 10)\n","\n","  def forward(self, x):\n","    print(\"Before\", x.size())\n","    x = F.relu(self.conv1(x))\n","    print(\"After conv1\", x.size())\n","    x = F.relu(self.conv2(x))\n","    print(\"After conv2\",x.size())\n","    x = x.view(-1, 10 * 12 * 12)\n","    print(\"After dimension reduction\", x.size())\n","    x = F.relu(self.fc1(x))\n","    print(\"After fc1\", x.size())\n","    x = self.fc2(x)\n","    print(\"After fc2\", x.size())\n","    return x\n","\n","cnn = CNN()\n","output = cnn(torch.randn(10, 1, 20, 20))  # Input Size: (10, 1, 20, 20)"],"metadata":{"id":"1nudN6YK1PS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","  def __init__(self):\n","    super(CNN, self).__init__()\n","    self.max_pool1 = nn.MaxPool2d(kernel_size=2)\n","    self.max_pool2 = nn.MaxPool2d(kernel_size=2)\n","    self.fc1 = nn.Linear(10 * 5 * 5, 50)\n","    self.fc2 = nn.Linear(50, 10)\n","\n","  def forward(self, x):\n","    print(\"Before\", x.size())\n","    x = F.relu(self.max_pool1(x))\n","    print(\"After max_pool1\", x.size())\n","    x = F.relu(self.max_pool2(x))\n","    print(\"After max_pool2\",x.size())\n","    x = x.view(-1, 10 * 5 * 5)\n","    print(\"After dimension reduction\", x.size())\n","    x = F.relu(self.fc1(x))\n","    print(\"After fc1\", x.size())\n","    x = self.fc2(x)\n","    print(\"After fc2\", x.size())\n","    return x\n","\n","cnn = CNN()\n","output = cnn(torch.randn(10, 1, 20, 20))"],"metadata":{"id":"aObCDbls1SId"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now let's construct a CNN model"],"metadata":{"id":"rgM7JyK_f6gu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms"],"metadata":{"id":"zCg8EJLdE_8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = datasets.MNIST('./data/', train=True, download=True, transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","    ]))\n","train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n","\n","test_data = datasets.MNIST('./data/', train=False, transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","    ]))\n","test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=50, shuffle=True)"],"metadata":{"id":"YmVVkwqfVS__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n","        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n","        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n","        self.fc2 = nn.Linear(500, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, kernel_size=2, stride=2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, kernel_size=2, stride=2)\n","\n","        x = x.view(-1, 4 * 4 * 50) # [batch_size, 50, 4, 4]\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"GP9NJ03qVS6N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cnn = CNN()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = optim.SGD(cnn.parameters(), lr=0.01)"],"metadata":{"id":"x4gAjLBrVS1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cnn.train()\n","for epoch in range(10):\n","  for index, (data, target) in enumerate(train_loader):\n","    optimizer.zero_grad()\n","    output = cnn(data)\n","    loss = criterion(output, target)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if index % 100 == 0:\n","      print(\"loss of {} epoch, {} index : {}\".format(epoch, index, loss.item()))"],"metadata":{"id":"A4jpLlXQVSv1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Quick Question\n","# Write an explanation of the code below (evaluation cell only!)"],"metadata":{"id":"lVuY2ZregO9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cnn.eval()\n","test_loss = 0\n","correct = 0\n","with torch.no_grad():\n","  for data, target in test_loader:\n","    output = cnn(data)\n","    test_loss += criterion(output, target).item()\n","    pred = output.argmax(dim=1, keepdim=True)\n","    correct += pred.eq(target.view_as(pred)).sum().item()\n","print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"metadata":{"id":"HzNEM_SzVecD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Question (optional)\n","# Try to adjust the hyperparameters (kernel size, # of conv layers, ...)\n","# and get better results than the baseline (default) model!!"],"metadata":{"id":"BsiXvSCx21ur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PtxmAAqCWr2D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"21_zoHW4ZpaS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### The End."],"metadata":{"id":"spc48rNHQnYL"}},{"cell_type":"markdown","source":["##### Please upload your Colab file @Github https://github.com/duneag2/intro-dl/tree/main/Assignment5\n","\n","*   First, make your folder by your name (e.g. seungeun)\n","*   Then upload your \"Jupyter Notebook\" file under that directory\n","\n","###### Need Help?\n","\n","\n","\n","*   Please refer to this link https://yeko90.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC-colab%EC%BD%94%EB%9E%A9%EC%97%90%EC%84%9C-%EC%95%95%EC%B6%95%ED%8C%8C%EC%9D%BC-%ED%92%80%EA%B8%B0 OR\n","*   Just save your Jupyter Notebook (.ipynb) file in here (colab) and upload via 'Add file' - 'Upload files' https://nthree.tistory.com/60"],"metadata":{"id":"iMNBVkjiS7D9"}},{"cell_type":"code","source":[],"metadata":{"id":"XzVGuer0S9Oh"},"execution_count":null,"outputs":[]}]}